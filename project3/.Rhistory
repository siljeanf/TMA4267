#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
#summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), data= college.train)
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = college.train$Terminal, Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, newdata=college.train) - college.train$Outstate)^2)
MSE_test_poly[d] = mean((predict(mod, newdata= college.test) - college.test$Outstate)^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
library(splines)
set.seed(1)
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main=paste("Smoothing spline, df = ", round(fit$df,3)), xlab="Expend", ylab="Outstate")
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
#training MSE
pred =  predict( fit, college.train$Expend)
MSE_spline_train <- mean((pred$y - college.train$Outstate )^2)
#test MSE
pred =  predict(fit, college.test$Expend)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
#MSE for polynomial regression models (1-10)
Degree <- seq(1,10,by=1)
MSE = MSE_train_poly
df <- data.frame(Degree, MSE)
kable(df)
library(randomForest)
set.seed(1)
# fit a model using random forests with a sufficiently large number of trees
rf.college <- randomForest(Outstate ~ .,data=college.train, mtry=5, ntrees=500)
#predict the response using test data
yhat.college <- predict(rf.college, newdata=college.test)
#test MSE
MSE_rf <- mean((yhat.college - college.test$Outstate)^2)
MSE <- c(mse_fwd,mse_lasso, MSE_rf)
Method <- c("Forward selection", "Lasso",
"Random Forest")
df <- data.frame(Method, MSE)
kable(df)
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download",
id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
#library(GGally)
#library(gridExtra)
#max(d.train$npreg)#max nr of pregnancies
#head(d.train) #overview of data
#ggpairs(d.train) + theme_minimal() #look at correlation between variables
#plot2 <- ggplot(d.train, aes(npreg)) + geom_histogram(binwidth = 2) + labs(title = "Histogram") + theme_minimal()
#plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
#grid.arrange(plot2, plot3, ncol=2, nrow = 1)
library(e1071)
set.seed(10111)
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svm.linear = svm(diabetes~.,
data = d.train,
kernel = 'linear')
#fit a support vector machine (radial boundary)
svm.radial = svm(diabetes~.,
data = d.train,
kernel = 'radial')
#CV to find the best parameters for each model
cv.linear <- tune(svm, diabetes ~ ., data=d.train, kernel = "linear",
ranges = list(cost = c(.001, .01 , .1, 1, 10, 100 ) ) )
cv.radial <- tune(svm, diabetes ~., data = d.train, kernel = "radial",
ranges = list(cost = c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4) ))
#fit new models with the optimalized parameters from CV
bestmod.linear = cv.linear$best.model
bestmod.radial = cv.radial$best.model
# Predict the response for the test set
pred.lin = predict(bestmod.linear, newdata = d.test)
pred.rad = predict(bestmod.radial, newdata = d.test)
# Confusion tables (0: no diabetes, 1: diabetes)
#for SVC (linear)
table(Prediction = pred.lin, Truth = d.test$diabetes)
#for SVM (radial)
table(Prediction = pred.rad, Truth = d.test$diabetes)
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are < or > than p = 0.5
glm.pred = rep("0",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="1"
#confusion table
table(Prediction = glm.pred, Truth = d.test$diabetes)
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData<-t(GeneData)
hc.eucl.complete=hclust(dist(GeneData,method="euclidian"), method="complete")
hc.eucl.average=hclust(dist(GeneData,method="euclidian"), method="average")
hc.eucl.single=hclust(dist(GeneData,method="euclidian"), method="single")
correlation<-dist(cor(t(GeneData)))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)
cutree(hc.eucl.complete, 2)
cutree(hc.eucl.average, 2)
cutree(hc.eucl.single, 2)
cutree(hc.corr.complete, 2)
cutree(hc.corr.average, 2)
cutree(hc.corr.single, 2)
color<-c(rep(1,20),rep(2,20))
pca_gene=prcomp(GeneData, scale=TRUE)
plot(pca_gene$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2",main="Plot of tissue samples using PCA")
legend("bottom", legend=c("Healthy", "Diseased"),
col=c("red", "blue"), cex=0.8,pch=16)
pve=100*pca_gene$sdev^2/sum(pca_gene$sdev^2)
cumsum(pve)[5]
gene_loading = pca_gene$rotation[, 1]
sort(abs(gene_loading), decreasing = TRUE)[1:10]
km.out = kmeans(GeneData,2, nstart = 20)
km.out$cluster
plot(pca_gene$x[,1:2], col=km.out$cluster,pch=19,xlab="Z1",ylab="Z2",main="Plot of tissue samples colored by K-means")
legend("bottom", legend=c("Healthy", "Diseased"),
col=c("black", "red"), cex=0.8,pch=16)
library(MASS)
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class,
true = d.corona$deceased)
sum <- 1926+31+39+14
rate <- (31+39)/sum
rate
library(MASS)
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class,
true = d.corona$deceased)
sum <- 1926+31+39+14
rate <- (31+39)/sum
glm.probs <- predict(model, data= d.corona, type="response")
#sort the probabilities for whether the observations are < or > than p = 0.5
glm.pred = rep("0",length(d.corona$deceased)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="1"
#confusion table
table(Prediction = glm.pred, Truth = d.corona$deceased)
table(Prediction = glm.pred, Truth = d.corona$deceased)
rate <- (44+1)/sum
rate
sum <- 1964 +44 +1 +1
rate1 <- (44+1)/sum
rate1
sum <- 1926+31+39+14
rate <- (31+39)/sum
rate
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO"  # google file ID
d.support <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id), header = T)
# We only look at complete cases
d.support <- d.support[complete.cases(d.support), ]
d.support <- d.support[d.support$totcst > 0, ]
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = dim(train_x)[2]) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
l2_mse
mse
?College
#make list of age, country and sex
a <- seq(20,100,1)
sexes = c("male", "female")
countries = c("France", "japan", "indonesia", "Korea")
#generate gridded data and predict
newdata = expand.grid(age=a,country=countries, sex=sexes)
p <- predict(model, newdata, type="response")
library(knitr)
library(rmarkdown)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")
# install.packages("MASS")
# install.packages("dplyr")
# install.packages("ISLR")
# install.packages("boot")
# install.packages("FactoMineR", dependencies = TRUE)
# install.packages("factoextra")
# install.packages("glmnet")
# install.packages("tree")
# install.packages("randomForest")
# install.packages("gbm")
# install.packages("keras")
# install.packages("pls")
# install.packages("gam")
library(ISLR)
library(keras)
library(ggplot2)
?College
set.seed(1)
College$Private = as.numeric(College$Private)
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#str(College)
#divide data into response and covariates
#training data
train_x <- college.train[,-9]
train_y <- college.train[,9]
#test data
test_x <- college.test[,-9]
test_y <- college.test[,9]
#we use the mean and std of the training data for both test and train set
mean <- apply(train_x , 2, mean)
std <- apply(train_x, 2, sd)
train_x <- scale(train_x, center = mean, scale = std)
test_x <- scale(test_x, center = mean, scale = std)
set.seed(123)
#define the model
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = dim(train_x)[2]) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
#compile
model %>% compile(optimizer = "rmsprop", loss = "mse", metrics = "mean_squared_error")
#train
history = model %>% fit(train_x, train_y, epochs = 300, batch_size = 8,
validation_split = 0.2) #20% of the training set as validation set
#plot
plot(history, metrics = "mean_squared_error", smooth = FALSE) +ggtitle("Training and Validation Error")
#test mse
error%<-% (model %>% evaluate(test_x, test_y))
mse <- error$mean_squared_error
#mse from compulsory 2
fwrd <- 4112680
lasso <-	3717020
rf<-2607985
set.seed(123)
#define model
l2_model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = dim(train_x)[2],
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 64, activation = 'relu',
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 1)
#compile
l2_model %>% compile(optimizer = "rmsprop", loss = "mse", metrics = "mean_squared_error")
#train
l2_history = l2_model %>% fit(train_x, train_y, epochs = 300, batch_size = 8,
validation_split = 0.2) #20% of the training set as validation set
#plot
plot(l2_history, metrics = "mean_squared_error", smooth = FALSE)+
ggtitle("Training and Validation Error with weight regularization")
#test mse
l2_error%<-% (l2_model %>% evaluate(test_x, test_y))
l2_mse <- l2_error$mean_squared_error
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
#1. the number of deceased for each country
t1 <- table(d.corona$country, d.corona$deceased)
colnames(t1) <- c("Died","Survived")
t1
#2. the number of deceased for each sex
t2<- table(d.corona$sex, d.corona$deceased)
colnames(t2) <- c("Died","Survived")
t2
#3. for each country: the number of deceased, separate for each sex.
t3_fr <- table ( d.corona[1:114,]$sex, d.corona[1:114,]$deceased )
names(dimnames(t3_fr)) <- list("", "France")
colnames(t3_fr) <- c("Died","Survived")
t3_fr
t3_ind <- table( d.corona[114:(114+69),]$sex, d.corona[114:(114+69),]$deceased )
names(dimnames(t3_ind)) <- list("", "Indonesia")
colnames(t3_ind) <- c("Died","Survived")
t3_ind
t3_ja <- table( d.corona[(114+69):(114+69+294),]$sex, d.corona[(114+69):(114+69+294),]$deceased )
names(dimnames(t3_ja)) <- list("", "Japan")
colnames(t3_ja) <- c("Died","Survived")
t3_ja
t3_ko <- table(d.corona[(114+69+294):(114+69+294+1451),]$sex, d.corona[(114+69+294):(114+69+294+1451),]$deceased)
names(dimnames(t3_ko)) <- list("", "Korea")
colnames(t3_ko) <- c("Died","Survived")
t3_ko
#logistic model
model= glm(deceased ~ ., data = d.corona, family = "binomial")
summary(model)
#odds ratio: increasing the covariate by ten units, we change odds ratio to die with
exp(coef(model)[3])*10
#prob to die for males vs. females
newdata_m= data.frame( sex= "male", age=seq(2,99,1), country="France")
newdata_f= data.frame(sex="female", age=seq(2,99,1), country="France")
pm <- predict(model, newdata_m, type="response")
pf <- predict(model, newdata_f, type="response")
pm/pf #per age
#make list of age, country and sex
a <- seq(20,100,1)
sexes = c("male", "female")
countries = c("France", "japan", "indonesia", "Korea")
#generate gridded data and predict
newdata = expand.grid(age=a,country=countries, sex=sexes)
p <- predict(model, newdata, type="response")
#plot
ggplot(d.corona, aes(x = age, y = deceased)) +
geom_line(data = newdata, aes(x=age, y=p, col=country, linetype=sex)) +
labs(title="Probability of dying of corona when deceased",
y="Probability of dying", x="Age")
#make a model only dependent on sex
mod1 <- glm(deceased ~ sex, data=d.corona, family = "binomial")
summary(mod1)$coef
mod2 <- glm(deceased ~ .+sex*age, data=d.corona, family = "binomial")
summary(mod2)$coef
mod3 <- glm(deceased ~ .+age*country, data=d.corona)
summary(mod3)$coef
library(MASS)
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class,
true = d.corona$deceased)
sum <- 1926+31+39+14
rate <- (31+39)/sum
rate
#find misclassification rate for logistic model
glm.probs <- predict(model, data= d.corona, type="response")
glm.pred = rep("0",length(d.corona$deceased))
glm.pred[glm.probs>0.5]="1"
table(Prediction = glm.pred, Truth = d.corona$deceased)
sum <- 1964 +44 +1 +1
rate1 <- (44+1)/sum
rate1
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO"  # google file ID
d.support <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id), header = T)
# We only look at complete cases
d.support <- d.support[complete.cases(d.support), ]
d.support <- d.support[d.support$totcst > 0, ]
ggplot( data=d.support[c("Private", "totcst")], aes(x=Private, y=totcst) )
head(d.support)
typeof(d.support)
head(d.support)
data(d.support)
head(d.support)
ggplot( data=d.support[c("age", "totcst")], aes(x=age, y=totcst) )
head(d.support)
par(mfrow=c(2,3))
ggplot( data=d.support[c("age", "totcst")], aes(x=age, y=totcst) ) +
geom_histogram()
ggplot( data=d.support, aes(x=age, y=totcst) ) +
geom_histogram()
head(d.support)
par(mfrow=c(2,3))
ggplot( data=d.support, aes(totcst) ) + geom_histogram()
ggplot( data=d.support, aes(totcst) ) + geom_histogram()
ggplot( data=d.support, aes(age) ) + geom_histogram()
ggplot( data=d.support, aes(dzgroup) ) + geom_histogram()
head(d.support)
ggplot( data=d.support, aes(totcst) ) + geom_histogram()
ggplot( data=d.support, aes(num.co) ) + geom_histogram()
ggplot( data=d.support, aes(scoma) ) + geom_histogram()
head(d.support)
head(d.support)
par(mfrow=c(3,3))
ggplot( data=d.support, aes(totcst) ) + geom_histogram()
ggplot( data=d.support, aes(num.co) ) + geom_histogram()
ggplot( data=d.support, aes(scoma) ) + geom_histogram()
ggplot( data=d.support, aes(edu) ) + geom_histogram()
ggplot( data=d.support, aes(meanbp) ) + geom_histogram()
ggplot( data=d.support, aes(hrt) ) + geom_histogram()
ggplot( data=d.support, aes(resp) ) + geom_histogram()
ggplot( data=d.support, aes(temp) ) + geom_histogram()
ggplot( data=d.support, aes(pafi) ) + geom_histogram()
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
#1. the number of deceased for each country
t1 <- table(d.corona$country, d.corona$deceased)
colnames(t1) <- c("Died","Survived")
t1
#2. the number of deceased for each sex
t2<- table(d.corona$sex, d.corona$deceased)
colnames(t2) <- c("Died","Survived")
t2
#3. for each country: the number of deceased, separate for each sex.
t3_fr <- table ( d.corona[1:114,]$sex, d.corona[1:114,]$deceased )
names(dimnames(t3_fr)) <- list("", "France")
colnames(t3_fr) <- c("Died","Survived")
t3_fr
t3_ind <- table( d.corona[114:(114+69),]$sex, d.corona[114:(114+69),]$deceased )
names(dimnames(t3_ind)) <- list("", "Indonesia")
colnames(t3_ind) <- c("Died","Survived")
t3_ind
t3_ja <- table( d.corona[(114+69):(114+69+294),]$sex, d.corona[(114+69):(114+69+294),]$deceased )
names(dimnames(t3_ja)) <- list("", "Japan")
colnames(t3_ja) <- c("Died","Survived")
t3_ja
t3_ko <- table(d.corona[(114+69+294):(114+69+294+1451),]$sex, d.corona[(114+69+294):(114+69+294+1451),]$deceased)
names(dimnames(t3_ko)) <- list("", "Korea")
colnames(t3_ko) <- c("Died","Survived")
t3_ko
#logistic model
model= glm(deceased ~ ., data = d.corona, family = "binomial")
summary(model)
#odds ratio: increasing the covariate by ten units, we change odds ratio to die with
exp(coef(model)[3])*10
#prob to die for males vs. females
newdata_m= data.frame( sex= "male", age=seq(2,99,1), country="France")
newdata_f= data.frame(sex="female", age=seq(2,99,1), country="France")
pm <- predict(model, newdata_m, type="response")
pf <- predict(model, newdata_f, type="response")
pm/pf #per age
#make list of age, country and sex
a <- seq(20,100,1)
sexes = c("male", "female")
countries = c("France", "japan", "indonesia", "Korea")
#generate gridded data and predict
newdata = expand.grid(age=a,country=countries, sex=sexes)
p <- predict(model, newdata, type="response")
#plot
ggplot(d.corona, aes(x = age, y = deceased)) +
geom_line(data = newdata, aes(x=age, y=p, col=country, linetype=sex)) +
labs(title="Probability of dying of corona when deceased",
y="Probability of dying", x="Age")
#make a model only dependent on sex
mod1 <- glm(deceased ~ sex, data=d.corona, family = "binomial")
summary(mod1)$coef
mod2 <- glm(deceased ~ .+sex*age, data=d.corona, family = "binomial")
summary(mod2)$coef
mod3 <- glm(deceased ~ .+age*country, data=d.corona)
summary(mod3)$coef
library(MASS)
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class,
true = d.corona$deceased)
sum <- 1926+31+39+14
rate <- (31+39)/sum
rate
#find misclassification rate for logistic model
glm.probs <- predict(model, data= d.corona, type="response")
glm.pred = rep("0",length(d.corona$deceased))
glm.pred[glm.probs>0.5]="1"
table(Prediction = glm.pred, Truth = d.corona$deceased)
sum <- 1964 +44 +1 +1
rate1 <- (44+1)/sum
rate1
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO"  # google file ID
d.support <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id), header = T)
